{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook controls\n",
    "remove_old_directories = False\n",
    "train_model = False\n",
    "evaluate_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# remove all directories from training and testing\n",
    "if (remove_old_directories):\n",
    "    shutil.rmtree('./results/gpt', ignore_errors=True)\n",
    "    shutil.rmtree('./tuned_model', ignore_errors=True)\n",
    "    shutil.rmtree('./tuned_tokenizer', ignore_errors=True)\n",
    "    shutil.rmtree('./tmp', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"facebook/opt-350m\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"openai-community/gpt2\"\n",
    "model_name = \"EleutherAI/gpt-neo-125m\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "def trim_to_last_punctuation(text):\n",
    "    # attempt to trim any of the cut off sentences\n",
    "    # reverse find the last punctuation\n",
    "    last_punctuation = -1\n",
    "    for p in ['.', '!', '?']:\n",
    "        last_punctuation = text.rfind(p)\n",
    "        if last_punctuation != -1:\n",
    "            break\n",
    "    if last_punctuation != -1:\n",
    "        text = text[:last_punctuation + 1]\n",
    "    return text\n",
    "\n",
    "def hashtag_format(text, summary):\n",
    "    prompt = f\"### Question: Summarize the following which is surrounded by quotes \\\"{text}\\\"\\n### Answer:\"\n",
    "    if (len(summary) > 0):\n",
    "        prompt += f\" {summary}\"\n",
    "    return prompt\n",
    "\n",
    "def format_text(text, summary):\n",
    "    limited_text = text[:max_length]\n",
    "    limited_text = trim_to_last_punctuation(limited_text)\n",
    "    return hashtag_format(limited_text, summary)\n",
    "\n",
    "response_template = \" ### Answer:\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paula\\.conda\\envs\\llama\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "if (train_model):\n",
    "    dataset = load_dataset(\"csv\", data_files=\"./datasets/podcast_with_summary_train.csv\", split=\"train\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    max_length = 1024\n",
    "\n",
    "    def formatting_prompts_func(example):\n",
    "        output_texts = []\n",
    "        for i in range(len(example['text'])):\n",
    "            txt = example['text'][i]\n",
    "            text = format_text(txt, example['summary'][i])\n",
    "            output_texts.append(text)\n",
    "        \n",
    "        return output_texts\n",
    "\n",
    "    collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./tmp\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=5e-5,\n",
    "        gradient_accumulation_steps=8\n",
    "        )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=dataset,\n",
    "        args=training_args,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model(\"./tuned_model\")\n",
    "    tokenizer.save_pretrained(\"./tuned_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576\n",
      "### Question: Summarize the following which is surrounded by quotes \"The following is a conversation with Andrew Ng, one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general. He cofounded Coursera and Google Brain, launched Deep Learning AI, Landing AI, and the AI Fund, and was the chief scientist at Baidu. As a Stanford professor and with Coursera and Deep Learning AI, he has helped educate and inspire millions of students, including me. This is the Artificial Intelligence Podcast.\"\n",
      "### Answer:\n",
      ".æ¥½The AI Fund focuses on building new companies from scratch that leverage AI technology. The Fund also aims to address challenges such as bias in AI and the ethical use of technology. The Fund helps startups integrate machine learning into their business and scale their AI efforts for maximum impact.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./datasets/podcast_with_summary_test.csv\")\n",
    "first_text = df['text'][0]\n",
    "expected_summary = df['summary'][0]\n",
    "\n",
    "prompt = format_text(first_text, \"\")\n",
    "\n",
    "print(len(prompt))\n",
    "print(prompt)\n",
    "print(expected_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_summary_from_response(response):\n",
    "    # find the start token\n",
    "    response_template_trimmed = response_template.strip()\n",
    "    start_token = response.find(response_template_trimmed)\n",
    "    if start_token == -1:\n",
    "        return \"\"\n",
    "    # trim from end of the start token to the end of the response\n",
    "    result = response[start_token + len(response_template_trimmed):]\n",
    "    result = trim_to_last_punctuation(result)    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "\n",
    "def create_generator(model_name, tokenizer_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\", truncation=True)\n",
    "\n",
    "def run_inference(generator, prompt, max_new_tokens, log_results=False):\n",
    "    generated_text = generator(prompt, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "    # generated_text = generator(prompt, max_length=1024, num_return_sequences=1)\n",
    "    if log_results:\n",
    "        print(generated_text[0][\"generated_text\"])\n",
    "        print(\"\\n\")\n",
    "    return parse_summary_from_response(generated_text[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Summarize the following which is surrounded by quotes \"The following is a conversation with Andrew Ng, one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general. He cofounded Coursera and Google Brain, launched Deep Learning AI, Landing AI, and the AI Fund, and was the chief scientist at Baidu. As a Stanford professor and with Coursera and Deep Learning AI, he has helped educate and inspire millions of students, including me. This is the Artificial Intelligence Podcast.\"\n",
      "### Answer: Andrew Ng, one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general, discusses the impact of artificial intelligence and its potential for improving education, innovation, and society. He discusses the\n",
      "\n",
      "\n",
      "Summary:  Andrew Ng, one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general, discusses the impact of artificial intelligence and its potential for improving education, innovation, and society.\n"
     ]
    }
   ],
   "source": [
    "generator_finetuned = create_generator(\"./tuned_model\", \"./tuned_tokenizer\")\n",
    "summary = run_inference(generator_finetuned, prompt, max_new_tokens, True)\n",
    "\n",
    "print(\"Summary: \" + summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Summarize the following which is surrounded by quotes \"The following is a conversation with Andrew Ng, one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general. He cofounded Coursera and Google Brain, launched Deep Learning AI, Landing AI, and the AI Fund, and was the chief scientist at Baidu. As a Stanford professor and with Coursera and Deep Learning AI, he has helped educate and inspire millions of students, including me. This is the Artificial Intelligence Podcast.\"\n",
      "### Answer: \"The following is a conversation with Andrew Ng, one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general. He cofounded Coursera and Google Brain, launched Deep Learning AI\n",
      "\n",
      "\n",
      "Summary:  \"The following is a conversation with Andrew Ng, one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general.\n"
     ]
    }
   ],
   "source": [
    "generator_original = create_generator(model_name, model_name)\n",
    "summary = run_inference(generator_original, prompt, max_new_tokens, True)\n",
    "\n",
    "print(\"Summary: \" + summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "# remove dir and all subdirs\n",
    "shutil.rmtree(\"./results/gpt\", ignore_errors=True)\n",
    "os.makedirs(\"./results/gpt\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import json\n",
    "\n",
    "def evaluate_df(df, generator, generator_tuned, max_new_tokens, name):\n",
    "    if (not evaluate_model):\n",
    "        return\n",
    "\n",
    "    os.makedirs(f\"./results/gpt/{name}\", exist_ok=True)\n",
    "\n",
    "    summaries_orig = []\n",
    "    summaries_tuned = []\n",
    "    reference_summaries = []\n",
    "    for i in range(len(df)):\n",
    "        text = df['text'][i]\n",
    "        expected_summary = df['summary'][i]\n",
    "        prompt = format_text(text, \"\")\n",
    "        summary = run_inference(generator, prompt, max_new_tokens)\n",
    "        summaries_orig.append(summary)\n",
    "        summary = run_inference(generator_tuned, prompt, max_new_tokens)\n",
    "        summaries_tuned.append(summary)\n",
    "        reference_summaries.append(expected_summary)\n",
    "\n",
    "    rouge = evaluate.load('rouge')\n",
    "    results = rouge.compute(predictions=summaries_orig, references=reference_summaries)\n",
    "    print(\"Original Model\")\n",
    "    print(results)\n",
    "    with open(f\"./results/gpt/{name}/gpt_basemodel_rouge_results.json\", \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    results = rouge.compute(predictions=summaries_tuned, references=reference_summaries)\n",
    "    print(\"Tuned Model\")\n",
    "    print(results)\n",
    "    with open(f\"./results/gpt/{name}/gpt_tuned_rouge_results.json\", \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'summary': reference_summaries,\n",
    "        'summary_orig': summaries_orig,\n",
    "        'summary_tuned': summaries_tuned\n",
    "    })\n",
    "    results_df.to_csv(f\"./results/gpt/{name}/summaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model\n",
      "{'rouge1': 0.0961155934905295, 'rouge2': 0.01812604849107987, 'rougeL': 0.07455373229382672, 'rougeLsum': 0.06043839586568904}\n",
      "Tuned Model\n",
      "{'rouge1': 0.07933848323030451, 'rouge2': 0.01512984165246287, 'rougeL': 0.05914637566199343, 'rougeLsum': 0.059186080745931305}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./datasets/podcast_with_summary_test.csv\")\n",
    "results_df = evaluate_df(df, generator_original, generator_finetuned, max_new_tokens, \"test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model\n",
      "{'rouge1': 0.09169151977706039, 'rouge2': 0.017574309230125557, 'rougeL': 0.07038032658676768, 'rougeLsum': 0.0593643230362514}\n",
      "Tuned Model\n",
      "{'rouge1': 0.08183894650298472, 'rouge2': 0.019099755191932608, 'rougeL': 0.061673981088244054, 'rougeLsum': 0.061566912565636206}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./datasets/podcast_with_summary_train.csv\")\n",
    "results_df = evaluate_df(df, generator_original, generator_finetuned, max_new_tokens, \"train_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
