{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"facebook/opt-350m\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"openai-community/gpt2\"\n",
    "model_name = \"EleutherAI/gpt-neo-125m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024\n",
    "\n",
    "def hashtag_format(text, summary):\n",
    "    prompt = f\"### Question: Summarize the following which is surrounded by quotes \\\"{text}\\\"\\n ### Answer:\"\n",
    "    if (len(summary) > 0):\n",
    "        prompt += f\" {summary}\"\n",
    "    return prompt\n",
    "\n",
    "def format_text(text, summary):\n",
    "    limited_text = text[:max_length]\n",
    "    return hashtag_format(limited_text, summary)\n",
    "\n",
    "response_template = \" ### Answer:\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paula\\.conda\\envs\\llama\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\paula\\.conda\\envs\\llama\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\paula\\.conda\\envs\\llama\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:32<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 32.2489, 'train_samples_per_second': 29.675, 'train_steps_per_second': 0.93, 'train_loss': 2.8078570048014324, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=2.8078570048014324, metrics={'train_runtime': 32.2489, 'train_samples_per_second': 29.675, 'train_steps_per_second': 0.93, 'total_flos': 335163907630080.0, 'train_loss': 2.8078570048014324, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"./datasets/podcast_with_summary.csv\", split=\"train\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_length = 1024\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['text'])):\n",
    "        txt = example['text'][i]\n",
    "        text = format_text(txt, example['summary'][i])\n",
    "        output_texts.append(text)\n",
    "    \n",
    "    return output_texts\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tmp\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=5e-5,\n",
    "    gradient_accumulation_steps=8\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tuned_tokenizer\\\\tokenizer_config.json',\n",
       " './tuned_tokenizer\\\\special_tokens_map.json',\n",
       " './tuned_tokenizer\\\\vocab.json',\n",
       " './tuned_tokenizer\\\\merges.txt',\n",
       " './tuned_tokenizer\\\\added_tokens.json',\n",
       " './tuned_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./tuned_model\")\n",
    "tokenizer.save_pretrained(\"./tuned_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107\n",
      "### Question: Summarize the following which is surrounded by quotes \"As part of MIT course 6S099, Artificial General Intelligence, I've gotten the chance to sit down with Max Tegmark. He is a professor here at MIT. He's a physicist, spent a large part of his career studying the mysteries of our cosmological universe. But he's also studied and delved into the beneficial possibilities and the existential risks of artificial intelligence. Amongst many other things, he is the cofounder of the Future of Life Institute, author of two books, both of which I highly recommend. First, Our Mathematical Universe. Second is Life 3.0. He's truly an out of the box thinker and a fun personality, so I really enjoy talking to him. If you'd like to see more of these videos in the future, please subscribe and also click the little bell icon to make sure you don't miss any videos. Also, Twitter, LinkedIn, agi.mit.edu if you wanna watch other lectures or conversations like this one. Better yet, go read Max's book, Life 3.0. Chapter seven on goals is my favorite. It's really where philosophy and eng\"\n",
      " ### Answer:\n",
      "AGI as described by MIT students revolve around Max Tegmark, a physicist and AGI expert. Discussions touch on the mysteries of the universe, the existential risks of AI, and the importance of aligning human values with AI. Defining intelligence, creativity, and the consciousness problem are key points of focus. AGI is seen as a tool for empowerment and progress, requiring careful consideration of values and ethics. Aiming for inclusive conversations and gradual implementation of basic ethical principles is emphasized. The importance of starting with known values and gradually expanding from there is highlighted, with an emphasis on building a consensus and aligning human values with AI systems.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./datasets/podcast_with_summary.csv\")\n",
    "first_text = df['text'][0]\n",
    "expected_summary = df['summary'][0]\n",
    "\n",
    "prompt = format_text(first_text, \"\")\n",
    "\n",
    "print(len(prompt))\n",
    "print(prompt)\n",
    "print(expected_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_summary_from_response(response):\n",
    "    # find the start token\n",
    "    start_token = response.find(response_template)\n",
    "    if start_token == -1:\n",
    "        return None\n",
    "    # trim from end of the start token to the end of the response\n",
    "    result = response[start_token + len(response_template):]\n",
    "    \n",
    "    # attempt to trim any of the cut off sentences\n",
    "    # reverse find the last punctuation\n",
    "    last_punctuation = -1\n",
    "    for p in ['.', '!', '?']:\n",
    "        last_punctuation = result.rfind(p)\n",
    "        if last_punctuation != -1:\n",
    "            break\n",
    "    if last_punctuation != -1:\n",
    "        result = result[:last_punctuation + 1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "### Question: Summarize the following which is surrounded by quotes \"As part of MIT course 6S099, Artificial General Intelligence, I've gotten the chance to sit down with Max Tegmark. He is a professor here at MIT. He's a physicist, spent a large part of his career studying the mysteries of our cosmological universe. But he's also studied and delved into the beneficial possibilities and the existential risks of artificial intelligence. Amongst many other things, he is the cofounder of the Future of Life Institute, author of two books, both of which I highly recommend. First, Our Mathematical Universe. Second is Life 3.0. He's truly an out of the box thinker and a fun personality, so I really enjoy talking to him. If you'd like to see more of these videos in the future, please subscribe and also click the little bell icon to make sure you don't miss any videos. Also, Twitter, LinkedIn, agi.mit.edu if you wanna watch other lectures or conversations like this one. Better yet, go read Max's book, Life 3.0. Chapter seven on goals is my favorite. It's really where philosophy and eng\"\n",
      " ### Answer: Max Tegmark, professor of physics at MIT, discusses the potential of artificial intelligence and the existential risks of artificial intelligence. He discusses the potential of artificial intelligence and\n",
      "\n",
      "Parsed Summary:  Max Tegmark, professor of physics at MIT, discusses the potential of artificial intelligence and the existential risks of artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./tuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tuned_tokenizer\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\", truncation=True)\n",
    "\n",
    "# Generate text\n",
    "generated_text = generator(prompt, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "\n",
    "print(len(generated_text))\n",
    "print(generated_text[0][\"generated_text\"])\n",
    "\n",
    "summary = parse_summary_from_response(generated_text[0][\"generated_text\"])\n",
    "print(\"\\nParsed Summary: \" + summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Summarize the following which is surrounded by quotes \"As part of MIT course 6S099, Artificial General Intelligence, I've gotten the chance to sit down with Max Tegmark. He is a professor here at MIT. He's a physicist, spent a large part of his career studying the mysteries of our cosmological universe. But he's also studied and delved into the beneficial possibilities and the existential risks of artificial intelligence. Amongst many other things, he is the cofounder of the Future of Life Institute, author of two books, both of which I highly recommend. First, Our Mathematical Universe. Second is Life 3.0. He's truly an out of the box thinker and a fun personality, so I really enjoy talking to him. If you'd like to see more of these videos in the future, please subscribe and also click the little bell icon to make sure you don't miss any videos. Also, Twitter, LinkedIn, agi.mit.edu if you wanna watch other lectures or conversations like this one. Better yet, go read Max's book, Life 3.0. Chapter seven on goals is my favorite. It's really where philosophy and eng\"\n",
      " ### Answer: Yes, I think it's a good idea to start with the goal of getting a PhD in Artificial General Intelligence. I'm not sure if that's a good\n",
      "\n",
      "Parsed Summary:  Yes, I think it's a good idea to start with the goal of getting a PhD in Artificial General Intelligence.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\", truncation=True)\n",
    "\n",
    "# Generate text\n",
    "generated_text = generator(prompt, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "\n",
    "print(generated_text[0][\"generated_text\"])\n",
    "\n",
    "summary = parse_summary_from_response(generated_text[0][\"generated_text\"])\n",
    "print(\"\\nParsed Summary: \" + summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
