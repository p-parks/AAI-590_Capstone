{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune a smaller GPT model\n",
    "\n",
    "Train a smaller GPT model to produce summaries like the gpt-4o model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook controls\n",
    "remove_old_directories = True\n",
    "train_model = True\n",
    "evaluate_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove artifacts from previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# remove all directories from training and testing\n",
    "if (remove_old_directories):\n",
    "    shutil.rmtree('./results/gpt', ignore_errors=True)\n",
    "    shutil.rmtree('./tuned_model', ignore_errors=True)\n",
    "    shutil.rmtree('./tuned_tokenizer', ignore_errors=True)\n",
    "    shutil.rmtree('./tmp', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"facebook/opt-350m\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"openai-community/gpt2\"\n",
    "model_name = \"EleutherAI/gpt-neo-125m\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/fine-tune the gpt model to produce summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paula\\.conda\\envs\\llama\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from SharedUtils import trim_to_max_length\n",
    "\n",
    "def hashtag_format(text, summary):\n",
    "    prompt = f\"### Question: Summarize the following which is surrounded by quotes \\\"{text}\\\"\\n### Answer:\"\n",
    "    if (len(summary) > 0):\n",
    "        prompt += f\" {summary}\"\n",
    "    return prompt\n",
    "\n",
    "def format_text(text, summary):\n",
    "    limited_text = trim_to_max_length(text)\n",
    "    return hashtag_format(limited_text, summary)\n",
    "\n",
    "response_template = \"### Answer:\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 255 examples [00:00, 1382.12 examples/s]\n",
      "c:\\Users\\paula\\.conda\\envs\\llama\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\paula\\.conda\\envs\\llama\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:00<00:00, 7083.00 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:00<00:00, 7727.05 examples/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:09<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 9.9907, 'train_samples_per_second': 76.572, 'train_steps_per_second': 2.402, 'train_loss': 2.3021672566731772, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "if (train_model):\n",
    "    dataset = load_dataset(\"csv\", data_files=\"./datasets/podcast_with_summary_train.csv\", split=\"train\")\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def formatting_prompts_func(example):\n",
    "        output_texts = []\n",
    "        for i in range(len(example['text_short'])):\n",
    "            txt = example['text_short'][i]\n",
    "            text = format_text(txt, example['summary'][i])\n",
    "            output_texts.append(text)\n",
    "        \n",
    "        return output_texts\n",
    "\n",
    "    collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./tmp\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        learning_rate=5e-5,\n",
    "        gradient_accumulation_steps=8\n",
    "        )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=dataset,\n",
    "        args=training_args,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model(\"./tuned_model\")\n",
    "    tokenizer.save_pretrained(\"./tuned_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference using the new fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065\n",
      "### Question: Summarize the following which is surrounded by quotes \"The following is a conversation with Andrew Ng, one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general. He cofounded Coursera and Google Brain, launched Deep Learning AI, Landing AI, and the AI Fund, and was the chief scientist at Baidu. As a Stanford professor and with Coursera and Deep Learning AI, he has helped educate and inspire millions of students, including me. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, give it five stars on Apple Podcast, support it on Patreon, or simply connect with me on Twitter at Lex Friedman, spelled F R I D M A N. As usual, I'll do one or two minutes of ads now and never any ads in the middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the listening experience. This show is presented by Cash App, the number one finance app in the App Store. When you get it, use code LEXPODCAST.\"\n",
      "### Answer:\n",
      "Andrew Ng discusses his groundbreaking work in AI education and innovation. Subscribe and support the Artificial Intelligence Podcast.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./datasets/podcast_with_summary_test.csv\")\n",
    "first_text = df['text'][0]\n",
    "expected_summary = df['summary'][0]\n",
    "\n",
    "prompt = format_text(first_text, \"\")\n",
    "\n",
    "print(len(prompt))\n",
    "print(prompt)\n",
    "print(expected_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SharedUtils import trim_to_last_punctuation\n",
    "\n",
    "def parse_summary_from_response(response):\n",
    "    # find the start token\n",
    "    response_template_trimmed = response_template.strip()\n",
    "    start_token = response.find(response_template_trimmed)\n",
    "    if start_token == -1:\n",
    "        return \"\"\n",
    "    # trim from end of the start token to the end of the response\n",
    "    result = response[start_token + len(response_template_trimmed):]\n",
    "    result = trim_to_last_punctuation(result)    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "def create_generator(model_name, tokenizer_name):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=\"cuda\", truncation=True)\n",
    "\n",
    "def run_inference(generator, prompt, max_new_tokens, log_results):\n",
    "    elapsed_time = 0\n",
    "    start_time = time.time()\n",
    "    generated_text = generator(prompt, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    if log_results:\n",
    "        print(generated_text[0][\"generated_text\"])\n",
    "        print(\"\\n\")\n",
    "    return parse_summary_from_response(generated_text[0][\"generated_text\"]), elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Summarize the following which is surrounded by quotes \"The following is a conversation with Andrew Ng, one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general. He cofounded Coursera and Google Brain, launched Deep Learning AI, Landing AI, and the AI Fund, and was the chief scientist at Baidu. As a Stanford professor and with Coursera and Deep Learning AI, he has helped educate and inspire millions of students, including me. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, give it five stars on Apple Podcast, support it on Patreon, or simply connect with me on Twitter at Lex Friedman, spelled F R I D M A N. As usual, I'll do one or two minutes of ads now and never any ads in the middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the listening experience. This show is presented by Cash App, the number one finance app in the App Store. When you get it, use code LEXPODCAST.\"\n",
      "### Answer: Andrew Ng discusses AI, Deep Learning, and AI funding, highlighting his impactful contributions to education, inspiring students, and inspiring innovation. Sponsored by Cash App, the number one finance app in the App Store. When you get it,\n",
      "\n",
      "\n",
      "Summary:  Andrew Ng discusses AI, Deep Learning, and AI funding, highlighting his impactful contributions to education, inspiring students, and inspiring innovation. Sponsored by Cash App, the number one finance app in the App Store.\n"
     ]
    }
   ],
   "source": [
    "generator_finetuned = create_generator(\"./tuned_model\", \"./tuned_tokenizer\")\n",
    "summary, elapsed = run_inference(generator_finetuned, prompt, max_new_tokens, True)\n",
    "\n",
    "print(\"Summary: \" + summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: Summarize the following which is surrounded by quotes \"The following is a conversation with Andrew Ng, one of the most impactful educators, researchers, innovators, and leaders in artificial intelligence and technology space in general. He cofounded Coursera and Google Brain, launched Deep Learning AI, Landing AI, and the AI Fund, and was the chief scientist at Baidu. As a Stanford professor and with Coursera and Deep Learning AI, he has helped educate and inspire millions of students, including me. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, give it five stars on Apple Podcast, support it on Patreon, or simply connect with me on Twitter at Lex Friedman, spelled F R I D M A N. As usual, I'll do one or two minutes of ads now and never any ads in the middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the listening experience. This show is presented by Cash App, the number one finance app in the App Store. When you get it, use code LEXPODCAST.\"\n",
      "### Answer: Yes, I do.\n",
      "\n",
      "### Answer: Yes, I do.\n",
      "\n",
      "### Answer: Yes, I do.\n",
      "\n",
      "### Answer: Yes, I do.\n",
      "\n",
      "### Answer: Yes, I do.\n",
      "\n",
      "###\n",
      "\n",
      "\n",
      "Summary:  Yes, I do.\n",
      "\n",
      "### Answer: Yes, I do.\n",
      "\n",
      "### Answer: Yes, I do.\n",
      "\n",
      "### Answer: Yes, I do.\n",
      "\n",
      "### Answer: Yes, I do.\n"
     ]
    }
   ],
   "source": [
    "generator_original = create_generator(model_name, model_name)\n",
    "summary, elapsed = run_inference(generator_original, prompt, max_new_tokens, True)\n",
    "\n",
    "print(\"Summary: \" + summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the results\n",
    "Compare the fine tuned model against the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "# remove dir and all subdirs\n",
    "shutil.rmtree(\"./results/gpt\", ignore_errors=True)\n",
    "os.makedirs(\"./results/gpt\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SharedUtils import evaluate_and_save_metrics\n",
    "\n",
    "def evaluate_df(df, generator, generator_tuned, max_new_tokens, name):\n",
    "    if (not evaluate_model):\n",
    "        return\n",
    "\n",
    "    os.makedirs(f\"./results/gpt/{name}\", exist_ok=True)\n",
    "\n",
    "    total_time_orig = 0\n",
    "    total_time_tuned = 0\n",
    "    summaries_orig = []\n",
    "    summaries_tuned = []\n",
    "    reference_summaries = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        text = df['text_short'][i]\n",
    "        expected_summary = df['summary'][i]\n",
    "        prompt = format_text(text, \"\")\n",
    "\n",
    "        summary, orig_time = run_inference(generator, prompt, max_new_tokens, False)\n",
    "        total_time_orig += orig_time\n",
    "        summaries_orig.append(summary)\n",
    "        \n",
    "        summary, tuned_time = run_inference(generator_tuned, prompt, max_new_tokens, False)\n",
    "        total_time_tuned += tuned_time\n",
    "        summaries_tuned.append(summary)\n",
    "        \n",
    "        reference_summaries.append(expected_summary)\n",
    "\n",
    "    print(\"Original Model\")\n",
    "    rouge_results, bleu_results = evaluate_and_save_metrics(\n",
    "        \"gpt\",\n",
    "        name,\n",
    "        \"gpt_basemodel\",\n",
    "        reference_summaries,\n",
    "        summaries_orig,\n",
    "        total_time_orig\n",
    "    )\n",
    "    print(rouge_results)\n",
    "    print(bleu_results)\n",
    "    print(f\"Total time (seconds): {total_time_orig}\")\n",
    "    print(f\"Total time (minutes): {total_time_orig / 60}\")\n",
    "\n",
    "    print(\"Tuned Model\")\n",
    "    rouge_results, bleu_results = evaluate_and_save_metrics(\n",
    "        \"gpt\",\n",
    "        name,\n",
    "        \"gpt_tunedmodel\",\n",
    "        reference_summaries,\n",
    "        summaries_tuned,\n",
    "        total_time_tuned\n",
    "    )\n",
    "    print(rouge_results)\n",
    "    print(bleu_results)\n",
    "    print(f\"Total time (seconds): {total_time_tuned}\")\n",
    "    print(f\"Total time (minutes): {total_time_tuned / 60}\")\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'summary': reference_summaries,\n",
    "        'summary_orig': summaries_orig,\n",
    "        'summary_tuned': summaries_tuned\n",
    "    })\n",
    "    results_df.to_csv(f\"./results/gpt/{name}/summaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model\n",
      "{'rouge1': 0.12072427950463045, 'rouge2': 0.03475482812254653, 'rougeL': 0.10260111806969327, 'rougeLsum': 0.09871728854113355}\n",
      "{'bleu': 0.022280129157186978, 'precisions': [0.14163903363334912, 0.027357107962872496, 0.011094301563287948, 0.005732152162584679], 'brevity_penalty': 1.0, 'length_ratio': 1.5706845238095237, 'translation_length': 2111, 'reference_length': 1344}\n",
      "Total time (seconds): 22.27033281326294\n",
      "Total time (minutes): 0.37117221355438235\n",
      "Tuned Model\n",
      "{'rouge1': 0.3413474058315226, 'rouge2': 0.1329295600864056, 'rougeL': 0.2926705947648975, 'rougeLsum': 0.2858684721017868}\n",
      "{'bleu': 0.06701134410781397, 'precisions': [0.3012170385395537, 0.11268343815513626, 0.04067245119305857, 0.014606741573033709], 'brevity_penalty': 1.0, 'length_ratio': 1.4672619047619047, 'translation_length': 1972, 'reference_length': 1344}\n",
      "Total time (seconds): 22.206040859222412\n",
      "Total time (minutes): 0.3701006809870402\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./datasets/podcast_with_summary_test.csv\")\n",
    "results_df = evaluate_df(df, generator_original, generator_finetuned, max_new_tokens, \"test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model\n",
      "{'rouge1': 0.11395003781241367, 'rouge2': 0.03189152393209295, 'rougeL': 0.09362619108654371, 'rougeLsum': 0.09284479701316148}\n",
      "{'bleu': 0.014962002400896252, 'precisions': [0.14165636588380717, 0.024250159540523293, 0.00712401055408971, 0.0020477815699658703], 'brevity_penalty': 1.0, 'length_ratio': 1.4970392301998519, 'translation_length': 8090, 'reference_length': 5404}\n",
      "Total time (seconds): 86.42873072624207\n",
      "Total time (minutes): 1.4404788454373678\n",
      "Tuned Model\n",
      "{'rouge1': 0.3451608840168716, 'rouge2': 0.14402487851848422, 'rougeL': 0.29473183017642296, 'rougeLsum': 0.29215741789876115}\n",
      "{'bleu': 0.07406983945577654, 'precisions': [0.30342187126106723, 0.11403184005923732, 0.045871559633027525, 0.018964836033188465], 'brevity_penalty': 1.0, 'length_ratio': 1.5466321243523315, 'translation_length': 8358, 'reference_length': 5404}\n",
      "Total time (seconds): 86.33567023277283\n",
      "Total time (minutes): 1.4389278372128804\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./datasets/podcast_with_summary_train.csv\")\n",
    "results_df = evaluate_df(df, generator_original, generator_finetuned, max_new_tokens, \"train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model\n",
      "{'rouge1': 0.11545779292410834, 'rouge2': 0.03249250870475698, 'rougeL': 0.09485084723429923, 'rougeLsum': 0.09372360966943771}\n",
      "{'bleu': 0.016755679752100527, 'precisions': [0.14165277913930008, 0.024893746205221615, 0.007947296873366098, 0.0028126352228472523], 'brevity_penalty': 1.0, 'length_ratio': 1.5117071724955542, 'translation_length': 10201, 'reference_length': 6748}\n",
      "Total time (seconds): 106.51281809806824\n",
      "Total time (minutes): 1.775213634967804\n",
      "Tuned Model\n",
      "{'rouge1': 0.344476235076636, 'rouge2': 0.14191126098656004, 'rougeL': 0.29394949113422897, 'rougeLsum': 0.29158740800529104}\n",
      "{'bleu': 0.07278369651905087, 'precisions': [0.30300096805421106, 0.11377484766756568, 0.044882377218324394, 0.01813720260322202], 'brevity_penalty': 1.0, 'length_ratio': 1.5308239478363959, 'translation_length': 10330, 'reference_length': 6748}\n",
      "Total time (seconds): 106.94402575492859\n",
      "Total time (minutes): 1.7824004292488098\n"
     ]
    }
   ],
   "source": [
    "# read the entire dataset for the final evaluation\n",
    "df = pd.read_csv(\"./datasets/podcast_with_summary.csv\")\n",
    "results_df = evaluate_df(df, generator_original, generator_finetuned, max_new_tokens, \"whole_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
